\documentclass{beamer}
\usepackage{beamerthemesplit}
\usepackage[style=authoryear,hyperref=true,citestyle=numeric]{biblatex} 
\usepackage{hyperref}
\usepackage{listings}
\usepackage[all]{xy}
\usepackage[utf8]{inputenc}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\useoutertheme{infolines}

\addbibresource{presentation}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\tiny\sffamily,
otherkeywords={self},             % Add keywords here
keywordstyle=\tiny\sffamily\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\tiny\sffamily\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

\title{Hadoop Hears a Who}
\author{Dan Colish}
\date{\today}

\begin{document}
\frame{\titlepage}

\section{What is Hadoop}

\frame
{
  \frametitle{Webcrawlers and storage}
  In 2004, the opensource webcrawler project \emph{Nutch} was looking for a
  storage and data processing solution.

  \begin{itemize}
    \item Provide fault tolerance
    \item Scale to storing millions of pages
    \item Avoid read/write bottlenecks
    \item Process large amounts of data on demand
  \end{itemize}

}


\frame
{
  \frametitle{HDFS: Hadoop's Filesystem}

  HDFS is a distributed, fault tolerant userspace filesystem.

  \begin{itemize}
    \item Composed of two core services: the namenode and the datanode
    \item Files are split into blocks
    \item Replicates blocks as they are written
    \item Balances blocks, all nodes share ~= partition of data
    \item Provides partition tolerance against datanode loss
  \end {itemize}

}

\frame
{
  \frametitle{Architecture of HDFS}
  \includegraphics[trim=0 0 0 15mm, clip, width=\linewidth]{hdfsarchitecture.png}
  \cite{hadoopdocs}
}

\frame 
{
  \frametitle{Namenode}
  \begin{itemize}
    \item Controls filesystem metadata and block placement
    \item Executes all non-idempotent file and directory operations
    \item Weak point of typical deployment (SPOF)
    \item Supervises datanode health
  \end{itemize}
}

\frame 
{
  \frametitle{Datanode}
  \begin{itemize}
    \item Serves read and write requests
    \item Executes all block operations
  \end{itemize}  
}


\frame
{
  \frametitle{MapReduce: Hadoop's Data Processing System}
  \begin{displaymath}
    \xymatrix{ 
      *+[Fe:blue]{JobClient} \ar[dr] & & \\
      & *+[F=:magenta]{JobTracker} \ar[dl] \ar[d] \ar[dr] \\
      *+[F]{TaskTracker} \ar[d] & *+[F]{TaskTracker} \ar[d] & *+[F]{TaskTracker} \ar[d] \\
      *+[Fe]{Task} & *+[Fe]{Task} & *+[Fe]{Task}
    }
  \end{displaymath}
}


\frame 
{
  \frametitle{JobTracker}
  \begin{itemize}
    \item Schedules Jobs
    \item Supervises TaskTrackers
  \end{itemize}
}

\frame 
{
  \frametitle{TaskTracker}
  \begin{itemize}
    \item Executes map and reduce tasks
    \item Tasks run in a child jvm
  \end{itemize}
}

\section{Developing MapReduce Jobs}

\frame
{
  \frametitle{The Lifecycle of a MapReduce job}
  \begin{itemize}
    \item Split: input files are split and distributes to mappers
    \item Map: Mappers group raw input to common keys
    \item Shuffle: Distrubuted mapper output is moved to single reducer based on
      keys
    \item Reduce: Groups values are combined by reducer and written to file
      output
  \end{itemize}
}

\begin{frame}[fragile]
  \frametitle{A Sample Job}
  \begin{lstlisting}[language=java,basicstyle=\tiny\sffamily]
    public class Sample extends Configured implements Tool {
      int run(String[] args) {
        ...
      }

      public static class MapClass extends Mapper<Text, Text, Text, Int> {
        @Override
        public void map(Text key, Text value, Mapper.Context context) {
          context.write(key, value);
        }
      }

      public static class Reduce extends Reducer<Text, Text, Text, IntWritable> {
        private IntWritable countWritable = new IntWritable(0);
        @Override
        public void map(Text key, Iterable<Text> values, Mapper.Context context) {
          for (Text _ : values) count++;
          countWritable.set(count);
          context.write(key, countWritable);
        }
      }
    }
  \end{lstlisting}
\end{frame}

\frame
{
  \frametitle{Other MapReduce langauges}
  \begin{itemize}
    \item Hadoop Streaming: Use with shell stdin/stdout
    \item Pig: Semi-structured scripting language
    \item Hive: SQL-like scripting language
    \item Cascalog: Prolog-like clojure meta language
  \end{itemize}
}

\frame
{
  \frametitle{Submitting and Monitoring Jobs}
  % \begin{itemize}
  % \end{itemize}

}

\section{Running a Cluster}

\frame 
{
  \frametitle{Hosted Hadoop}
  \begin{itemize}
    \item Cluster mgmt is time consuming and expensive.
    \item Off load to service providers when possible
    \item Use Amazon Elastic MapReduce (EMR)
  \end{itemize}
}

\begin{frame}[fragile]
  \frametitle{Submitting to EMR}
  \begin{python}
    import datetime, os
    import boto
    from boto.emr.instance_group import InstanceGroup
    from boto.emr.step import InstallPigStep, PigStep

    conn = boto.connect_emr()
    instance_groups = [
        InstanceGroup(1, 'MASTER', 'm1.small', 'SPOT', 'master@0.10', '0.10'),
        InstanceGroup(2, 'CORE', 'm1.small', 'SPOT', 'master@0.10', '0.10'),
        ]

    pig_file = 's3://elasticmapreduce/samples/pig-apache/do-reports2.pig'
    INPUT = 's3://elasticmapreduce/samples/pig-apache/input/'
    OUTPUT = ('s3://org.unencrypted.emr.output/apache_sample/%s' %
              datetime.datetime.utcnow().strftime("%s"))
    pig_args = ['-p', 'INPUT=%s' % INPUT,
                '-p', 'OUTPUT=%s' % OUTPUT]

    pig_step = PigStep('Process Reports', pig_file, pig_args=pig_args)
    steps = [InstallPigStep(), pig_step]

    job_id = conn.run_jobflow(name='sample apache report',
        ec2_keyname=os.getenv("EC2_KEY_NAME"), steps=steps,
        log_uri="s3://org.unencrypted.emr.log/sampleflow_logs", enable_debugging=True,
        ami_version="latest", instance_groups=instance_groups, keep_alive=True)
  \end{python}
\end{frame}

\frame
{
  \frametitle{References}
  \printbibliography
}

\end{document}

% HA setup wtih hdfs
% http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithQJM.html
